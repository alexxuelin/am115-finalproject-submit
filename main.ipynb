{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country                                        description  \\\n",
       "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
       "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
       "4           4  France  This is the top wine from La Bégude, named aft...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "2         Special Selected Late Harvest      96   90.0      California   \n",
       "3                               Reserve      96   65.0          Oregon   \n",
       "4                            La Brûlade      95   66.0        Provence   \n",
       "\n",
       "            region_1           region_2             variety  \\\n",
       "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
       "1               Toro                NaN       Tinta de Toro   \n",
       "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
       "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
       "4             Bandol                NaN  Provence red blend   \n",
       "\n",
       "                    winery  \n",
       "0                    Heitz  \n",
       "1  Bodega Carmen Rodríguez  \n",
       "2                 Macauley  \n",
       "3                    Ponzi  \n",
       "4     Domaine de la Bégude  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/winemag-data_first150k.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89105, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated('description',keep=False)].sort_values('description').head(5)\n",
    "data = data.drop_duplicates('description')\n",
    "data = data[pd.notnull(data.price)]\n",
    "data = data[pd.notnull(data.country)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aromas that come together are those of scorched earth, grilled porcini, roasted coffee bean and a bit of burnt rubber. The mature palate offers dried cherry, blackberry confiture, mocha, vanilla and a hint of game alongside tongue-drying tannins that clench the finish. Give this a few years to unfold then drink sooner rather than later.\n",
      "Intense notes of crushed stone and slate permeate this stately off-dry Riesling. Pert lemon and tangerine flavors are zippy and fresh, but the wine is more a showcase for its brisk, stony style.\n"
     ]
    }
   ],
   "source": [
    "print data.description[400]\n",
    "print data.description[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((59991, 2), (19997, 2), (59991,), (19997,))\n",
      "['Aglianico', 'Albari\\xc3\\xb1o', 'Barbera', 'Bordeaux-style Red Blend', 'Bordeaux-style White Blend', 'Cabernet Franc', 'Cabernet Sauvignon', 'Carmen\\xc3\\xa8re', 'Champagne Blend', 'Chardonnay', 'Chenin Blanc', 'Corvina, Rondinella, Molinara', 'Gamay', 'Garnacha', 'Gew\\xc3\\xbcrztraminer', 'Glera', 'Grenache', 'Gr\\xc3\\xbcner Veltliner', 'Malbec', 'Meritage', 'Merlot', 'Moscato', 'Nebbiolo', \"Nero d'Avola\", 'Petite Sirah', 'Pinot Blanc', 'Pinot Grigio', 'Pinot Gris', 'Pinot Noir', 'Port', 'Portuguese Red', 'Portuguese White', 'Prosecco', 'Red Blend', 'Rh\\xc3\\xb4ne-style Red Blend', 'Rh\\xc3\\xb4ne-style White Blend', 'Riesling', 'Ros\\xc3\\xa9', 'Sangiovese', 'Sangiovese Grosso', 'Sauvignon Blanc', 'Shiraz', 'Sparkling Blend', 'Syrah', 'Tempranillo', 'Tempranillo Blend', 'Torront\\xc3\\xa9s', 'Verdejo', 'Viognier', 'White Blend', 'Zinfandel']\n"
     ]
    }
   ],
   "source": [
    "# leave everything except for description out\n",
    "X = data.drop(['Unnamed: 0','country','designation','points','province','region_1','region_2','variety','winery'], axis = 1)\n",
    "y = data.variety\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "wine =data.variety.unique().tolist()\n",
    "wine.sort()\n",
    "print wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the train test split \n",
    "with open('X_train', 'wb') as fp1:\n",
    "    pickle.dump(X_train, fp1)\n",
    "\n",
    "with open('X_test', 'wb') as fp2:\n",
    "    pickle.dump(X_test, fp2)\n",
    "    \n",
    "with open('y_train', 'wb') as fp3:\n",
    "    pickle.dump(y_train, fp3)\n",
    "\n",
    "with open('y_test', 'wb') as fp4:\n",
    "    pickle.dump(y_test, fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reimport \n",
    "with open ('X_train', 'rb') as fp1:\n",
    "    X_train_pk = pickle.load(fp1)\n",
    "\n",
    "with open ('X_test', 'rb') as fp2:\n",
    "    X_test_pk = pickle.load(fp2)\n",
    "    \n",
    "with open ('y_train', 'rb') as fp3:\n",
    "    y_train_pk = pickle.load(fp3)\n",
    "\n",
    "with open ('y_test', 'rb') as fp4:\n",
    "    y_test_pk = pickle.load(fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aglianico', 'albari\\xc3\\xb1o', 'barbera', 'blanc', 'blend', 'bordeaux-style', 'cabernet', 'carmen\\xc3\\xa8re', 'champagne', 'chardonnay']\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "output = set()\n",
    "for x in data.variety:\n",
    "    x = x.lower()\n",
    "    x = x.split()\n",
    "    for y in x:\n",
    "        output.add(y)\n",
    "\n",
    "variety_list =sorted(output)\n",
    "print variety_list[:10]\n",
    "print len(variety_list) # too many classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59991, 24953)\n",
      "(19997, 24953)\n"
     ]
    }
   ],
   "source": [
    "extras = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', 'cab',\"%\"]\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(variety_list)\n",
    "stop.update(extras)\n",
    "\n",
    "# features\n",
    "vect = CountVectorizer(stop_words = stop)\n",
    "X_train_dtm = vect.fit_transform(X_train.description)\n",
    "price = X_train.price.values[:,None]\n",
    "X_train_dtm = hstack((X_train_dtm, price))\n",
    "print X_train_dtm.shape\n",
    "\n",
    "X_test_dtm = vect.transform(X_test.description)\n",
    "price_test = X_test.price.values[:,None]\n",
    "X_test_dtm = hstack((X_test_dtm, price_test))\n",
    "print X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the train and test data for all different methods\n",
    "# save words into importable object\n",
    "with open('data_train', 'wb') as fp5:\n",
    "    pickle.dump(X_train_dtm, fp5)\n",
    "\n",
    "with open('data_test', 'wb') as fp6:\n",
    "    pickle.dump(X_test_dtm, fp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read back\n",
    "with open ('data_train', 'rb') as fp5:\n",
    "    X_train_dtm = pickle.load(fp5)\n",
    "\n",
    "with open ('data_test', 'rb') as fp6:\n",
    "    X_test_dtm = pickle.load(fp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy Score:', 52.945721460305194, '%')\n",
      "                   actual           predicted\n",
      "0              Chardonnay          Chardonnay\n",
      "1               Aglianico           Aglianico\n",
      "2              Pinot Noir          Pinot Noir\n",
      "3               Carmenère              Malbec\n",
      "4      Cabernet Sauvignon  Cabernet Sauvignon\n",
      "5   Rhône-style Red Blend           Red Blend\n",
      "6               Carmenère            Garnacha\n",
      "7         Sauvignon Blanc     Sauvignon Blanc\n",
      "8                  Shiraz  Cabernet Sauvignon\n",
      "9              Pinot Noir          Pinot Noir\n",
      "10               Riesling            Riesling\n",
      "11             Pinot Noir          Pinot Noir\n",
      "12        Sauvignon Blanc     Sauvignon Blanc\n",
      "13               Nebbiolo            Nebbiolo\n",
      "14             Sangiovese           Red Blend\n",
      "15           Nero d'Avola           Red Blend\n",
      "16                  Syrah  Cabernet Sauvignon\n",
      "17               Albariño            Albariño\n",
      "18               Viognier          Pinot Gris\n",
      "19     Cabernet Sauvignon  Cabernet Sauvignon\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for z in wine:\n",
    "    model = LogisticRegression()\n",
    "    y = y_train == z\n",
    "    model.fit(X_train_dtm, y)\n",
    "    models[z] = model\n",
    "testing_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "# print score\n",
    "for variety in wine:\n",
    "    testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n",
    "\n",
    "predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "comparison = pd.DataFrame({'actual':y_test.values, 'predicted':predicted_wine.values})\n",
    "\n",
    "print('Accuracy Score:',accuracy_score(comparison.actual, comparison.predicted)*100,\"%\")\n",
    "print comparison.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Set Accuracy Score:', 84.552682902435365, '%')\n",
      "('Test Set Accuracy Score:', 54.438165724858735, '%')\n",
      "                actual                 predicted\n",
      "0           Pinot Noir                Pinot Noir\n",
      "1   Cabernet Sauvignon        Cabernet Sauvignon\n",
      "2      Champagne Blend                Chardonnay\n",
      "3          Tempranillo                    Malbec\n",
      "4      Champagne Blend                Chardonnay\n",
      "5     Grüner Veltliner                 Red Blend\n",
      "6               Shiraz        Cabernet Sauvignon\n",
      "7            Red Blend                 Red Blend\n",
      "8                Syrah                     Syrah\n",
      "9            Red Blend        Cabernet Sauvignon\n",
      "10          Chardonnay                Chardonnay\n",
      "11          Pinot Noir                Pinot Noir\n",
      "12            Grenache                Sangiovese\n",
      "13            Riesling                  Riesling\n",
      "14          Chardonnay                Chardonnay\n",
      "15           Red Blend                 Red Blend\n",
      "16          Pinot Noir                Pinot Noir\n",
      "17            Nebbiolo                Sangiovese\n",
      "18           Zinfandel  Bordeaux-style Red Blend\n",
      "19          Chardonnay                Chardonnay\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for z in wine:\n",
    "    model = LogisticRegression()\n",
    "    y = y_train == z\n",
    "    model.fit(X_train_dtm, y)\n",
    "    models[z] = model\n",
    "testing_probs = pd.DataFrame(columns = wine)\n",
    "training_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "# predict for training set \n",
    "for variety in wine:\n",
    "    training_probs[variety] = models[variety].predict_proba(X_train_dtm)[:,1]\n",
    "train_predicted_wine = training_probs.idxmax(axis=1)\n",
    "\n",
    "# predict for test set\n",
    "for variety in wine:\n",
    "    testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n",
    "test_predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "train_comparison = pd.DataFrame({'actual':y_train.values, 'predicted':train_predicted_wine.values})\n",
    "test_comparison = pd.DataFrame({'actual':y_test.values, 'predicted':test_predicted_wine.values})\n",
    "\n",
    "print('Training Set Accuracy Score:',accuracy_score(train_comparison.actual, train_comparison.predicted)*100,\"%\")\n",
    "print('Test Set Accuracy Score:',accuracy_score(test_comparison.actual, test_comparison.predicted)*100,\"%\")\n",
    "print test_comparison.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print len(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target names\n",
    "target_names = ['Aglianico', 'Albarino', 'Barbera', 'Bordeaux-style Red Blend', 'Bordeaux-style White Blend',\n",
    " 'Cabernet Franc', 'Cabernet Sauvignon', 'Carmenere', 'Champagne Blend', 'Chardonnay', 'Chenin Blanc', \n",
    " 'Corvina, Rondinella, Molinara', 'Gamay', 'Garnacha', 'Gewurztraminer', 'Glera', 'Grenache', \n",
    " 'Gruner Veltliner ', 'Malbec', 'Meritage', 'Merlot', 'Moscato', 'Nebbiolo', \"Nero d'Avola\", \n",
    " 'Petite Sirah', 'Pinot Blanc', 'Pinot Grigio', 'Pinot Gris', 'Pinot Noir', 'Port', 'Portuguese Red', \n",
    " 'Portuguese White', 'Prosecco', 'Red Blend', 'Rhone-style Red Blend', 'Rhone-style White Blend', 'Riesling', \n",
    " 'Rose', 'Sangiovese', 'Sangiovese Grosso', 'Sauvignon Blanc', 'Shiraz', \n",
    " 'Sparkling Blend', 'Syrah', 'Tempranillo', 'Tempranillo Blend', 'Torrontes', 'Verdejo', \n",
    " 'Viognier', 'White Blend', 'Zinfandel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                    Aglianico       0.60      0.21      0.31        57\n",
      "                     Albarino       0.62      0.49      0.55        77\n",
      "                      Barbera       0.58      0.26      0.36       117\n",
      "     Bordeaux-style Red Blend       0.52      0.51      0.52       803\n",
      "   Bordeaux-style White Blend       0.51      0.28      0.36        94\n",
      "               Cabernet Franc       0.17      0.05      0.08       200\n",
      "           Cabernet Sauvignon       0.46      0.62      0.53      2009\n",
      "                    Carmenere       0.47      0.35      0.40       121\n",
      "              Champagne Blend       0.58      0.40      0.47       168\n",
      "                   Chardonnay       0.64      0.84      0.73      2172\n",
      "                 Chenin Blanc       0.37      0.18      0.24        89\n",
      "Corvina, Rondinella, Molinara       0.79      0.71      0.75       227\n",
      "                        Gamay       0.71      0.35      0.47        68\n",
      "                     Garnacha       0.07      0.02      0.03        50\n",
      "               Gewurztraminer       0.74      0.51      0.61       158\n",
      "                        Glera       0.70      0.61      0.65        76\n",
      "                     Grenache       0.15      0.02      0.04        92\n",
      "            Gruner Veltliner        0.65      0.51      0.57       146\n",
      "                       Malbec       0.32      0.29      0.30       489\n",
      "                     Meritage       0.40      0.04      0.07        49\n",
      "                       Merlot       0.27      0.21      0.23       800\n",
      "                      Moscato       0.71      0.40      0.51        80\n",
      "                     Nebbiolo       0.74      0.63      0.68       233\n",
      "                 Nero d'Avola       0.57      0.38      0.45        56\n",
      "                 Petite Sirah       0.38      0.17      0.24       155\n",
      "                  Pinot Blanc       0.30      0.09      0.14        64\n",
      "                 Pinot Grigio       0.47      0.33      0.39       214\n",
      "                   Pinot Gris       0.34      0.21      0.26       216\n",
      "                   Pinot Noir       0.61      0.77      0.68      2156\n",
      "                         Port       0.76      0.44      0.56       160\n",
      "               Portuguese Red       0.57      0.59      0.58       304\n",
      "             Portuguese White       0.65      0.53      0.59       136\n",
      "                     Prosecco       0.60      0.54      0.57        74\n",
      "                    Red Blend       0.44      0.48      0.46      1504\n",
      "        Rhone-style Red Blend       0.65      0.40      0.49       272\n",
      "      Rhone-style White Blend       0.67      0.57      0.61        67\n",
      "                     Riesling       0.65      0.74      0.69       847\n",
      "                         Rose       0.64      0.67      0.65       430\n",
      "                   Sangiovese       0.50      0.40      0.45       475\n",
      "            Sangiovese Grosso       0.87      0.71      0.78       156\n",
      "              Sauvignon Blanc       0.62      0.69      0.65       928\n",
      "                       Shiraz       0.48      0.41      0.44       303\n",
      "              Sparkling Blend       0.60      0.51      0.55       295\n",
      "                        Syrah       0.39      0.34      0.36       916\n",
      "                  Tempranillo       0.31      0.32      0.31       356\n",
      "            Tempranillo Blend       0.21      0.06      0.10       127\n",
      "                    Torrontes       0.59      0.46      0.51        57\n",
      "                      Verdejo       0.30      0.11      0.16        53\n",
      "                     Viognier       0.50      0.32      0.39       213\n",
      "                  White Blend       0.59      0.40      0.48       456\n",
      "                    Zinfandel       0.68      0.62      0.64       632\n",
      "\n",
      "                  avg / total       0.53      0.54      0.53     19997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "y_true = y_test.values\n",
    "y_pred = test_predicted_wine.values \n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Logistic Regression \n",
    "- different levels of regularization\n",
    "- L1 vs. L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The regularization constant is', 0.001)\n",
      "('Training Set Accuracy Score:', 35.218616125752192, '%')\n",
      "('Test Set Accuracy Score:', 34.045106766014904, '%')\n",
      "('The regularization constant is', 0.01)\n",
      "('Training Set Accuracy Score:', 49.020686436298774, '%')\n",
      "('Test Set Accuracy Score:', 45.996899534930243, '%')\n",
      "('The regularization constant is', 0.1)\n",
      "('Training Set Accuracy Score:', 65.588171559067206, '%')\n",
      "('Test Set Accuracy Score:', 53.798069710456566, '%')\n",
      "('The regularization constant is', 1)\n",
      "('Training Set Accuracy Score:', 84.552682902435365, '%')\n",
      "('Test Set Accuracy Score:', 54.438165724858735, '%')\n",
      "('The regularization constant is', 10)\n",
      "('Training Set Accuracy Score:', 94.709206380957141, '%')\n",
      "('Test Set Accuracy Score:', 50.457568635295289, '%')\n",
      "('The regularization constant is', 100)\n",
      "('Training Set Accuracy Score:', 97.627977529962834, '%')\n",
      "('Test Set Accuracy Score:', 46.056908536280446, '%')\n",
      "('The regularization constant is', 1000)\n",
      "('Training Set Accuracy Score:', 98.191395375973073, '%')\n",
      "('Test Set Accuracy Score:', 44.146621993298993, '%')\n"
     ]
    }
   ],
   "source": [
    "# L2 Penalty\n",
    "params = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for p in params:\n",
    "    \n",
    "    models = {}\n",
    "    for z in wine:\n",
    "        model = LogisticRegression(penalty='l2',C=p)\n",
    "        y = y_train == z\n",
    "        model.fit(X_train_dtm, y)\n",
    "        models[z] = model\n",
    "    testing_probs = pd.DataFrame(columns = wine)\n",
    "    training_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "    # predict for training set \n",
    "    for variety in wine:\n",
    "        training_probs[variety] = models[variety].predict_proba(X_train_dtm)[:,1]\n",
    "    train_predicted_wine = training_probs.idxmax(axis=1)\n",
    "\n",
    "    # predict for test set\n",
    "    for variety in wine:\n",
    "        testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n",
    "    test_predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "    train_comparison = pd.DataFrame({'actual':y_train.values, 'predicted':train_predicted_wine.values})\n",
    "    test_comparison = pd.DataFrame({'actual':y_test.values, 'predicted':test_predicted_wine.values})\n",
    "    \n",
    "    print('The regularization constant is', p)\n",
    "    print('Training Set Accuracy Score:',accuracy_score(train_comparison.actual, train_comparison.predicted)*100,\"%\")\n",
    "    print('Test Set Accuracy Score:',accuracy_score(test_comparison.actual, test_comparison.predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The regularization constant is', 0.001)\n",
      "('Training Set Accuracy Score:', 17.707656148422263, '%')\n",
      "('Test Set Accuracy Score:', 17.357603640546081, '%')\n",
      "('The regularization constant is', 0.01)\n",
      "('Training Set Accuracy Score:', 37.82567385107766, '%')\n",
      "('Test Set Accuracy Score:', 36.885532829924486, '%')\n",
      "('The regularization constant is', 0.1)\n",
      "('Training Set Accuracy Score:', 53.468020203030456, '%')\n",
      "('Test Set Accuracy Score:', 50.632594889233381, '%')\n",
      "('The regularization constant is', 1)\n",
      "('Training Set Accuracy Score:', 74.346151922788422, '%')\n",
      "('Test Set Accuracy Score:', 55.068260239035858, '%')\n",
      "('The regularization constant is', 10)\n",
      "('Training Set Accuracy Score:', 96.267773499358228, '%')\n",
      "('Test Set Accuracy Score:', 48.497274591188678, '%')\n",
      "('The regularization constant is', 100)\n",
      "('Training Set Accuracy Score:', 98.731476388124889, '%')\n",
      "('Test Set Accuracy Score:', 42.396359453918087, '%')\n",
      "('The regularization constant is', 1000)\n",
      "('Training Set Accuracy Score:', 99.318231067993537, '%')\n",
      "('Test Set Accuracy Score:', 38.300745111766766, '%')\n"
     ]
    }
   ],
   "source": [
    "# L1 PENAULTY\n",
    "params = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for p in params:\n",
    "    \n",
    "    models = {}\n",
    "    for z in wine:\n",
    "        model = LogisticRegression(penalty='l1',C = p)\n",
    "        y = y_train == z\n",
    "        model.fit(X_train_dtm, y)\n",
    "        models[z] = model\n",
    "    testing_probs = pd.DataFrame(columns = wine)\n",
    "    training_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "    # predict for training set \n",
    "    for variety in wine:\n",
    "        training_probs[variety] = models[variety].predict_proba(X_train_dtm)[:,1]\n",
    "    train_predicted_wine = training_probs.idxmax(axis=1)\n",
    "\n",
    "    # predict for test set\n",
    "    for variety in wine:\n",
    "        testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n",
    "    test_predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "    train_comparison = pd.DataFrame({'actual':y_train.values, 'predicted':train_predicted_wine.values})\n",
    "    test_comparison = pd.DataFrame({'actual':y_test.values, 'predicted':test_predicted_wine.values})\n",
    "    \n",
    "    print('The regularization constant is', p)\n",
    "    print('Training Set Accuracy Score:',accuracy_score(train_comparison.actual, train_comparison.predicted)*100,\"%\")\n",
    "    print('Test Set Accuracy Score:',accuracy_score(test_comparison.actual, test_comparison.predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Improvements\n",
    "- Regularization?\n",
    "- Fewer features for text (is it fixable by regularization though)?\n",
    "- More predicting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 535.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 685.0, 174.0, 175.0, 1200.0, 178.0, 179.0, 180.0, 182.0, 184.0, 185.0, 187.0, 189.0, 190.0, 191.0, 193.0, 194.0, 195.0, 196.0, 197.0, 199.0, 200.0, 202.0, 203.0, 204.0, 205.0, 206.0, 208.0, 1400.0, 210.0, 211.0, 212.0, 740.0, 214.0, 215.0, 216.0, 217.0, 220.0, 224.0, 225.0, 226.0, 227.0, 228.0, 550.0, 230.0, 600.0, 530.0, 235.0, 236.0, 237.0, 238.0, 240.0, 243.0, 757.0, 246.0, 248.0, 249.0, 250.0, 639.0, 252.0, 253.0, 255.0, 256.0, 588.0, 260.0, 520.0, 775.0, 265.0, 266.0, 268.0, 269.0, 270.0, 271.0, 273.0, 468.0, 275.0, 1300.0, 279.0, 280.0, 281.0, 285.0, 288.0, 290.0, 292.0, 294.0, 295.0, 299.0, 300.0, 303.0, 351.0, 307.0, 310.0, 312.0, 315.0, 316.0, 317.0, 319.0, 320.0, 139.0, 325.0, 328.0, 330.0, 848.0, 850.0, 625.0, 345.0, 349.0, 350.0, 229.0, 354.0, 360.0, 231.0, 365.0, 599.0, 574.0, 375.0, 376.0, 660.0, 380.0, 385.0, 900.0, 390.0, 391.0, 398.0, 400.0, 323.0, 173.0, 580.0, 411.0, 525.0, 419.0, 428.0, 430.0, 440.0, 445.0, 245.0, 448.0, 450.0, 455.0, 1100.0, 460.0, 463.0, 612.0, 467.0, 980.0, 2013.0, 800.0, 486.0, 1000.0, 495.0, 500.0, 545.0, 596.0, 510.0])\n"
     ]
    }
   ],
   "source": [
    "print set(X_train_dtm.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25415\n",
      "[u'00', u'000', u'002', u'008', u'01', u'01s', u'02', u'02s', u'03', u'03s', u'04', u'04s', u'05', u'056', u'05s', u'06', u'061', u'064', u'06s', u'07', u'07s', u'08', u'080', u'082', u'08s', u'09', u'093', u'10', u'100', u'1000', u'100g', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'10th', u'11', u'110', u'1100s', u'111', u'112', u'114', u'115', u'117', u'1170', u'11th', u'12', u'120', u'1200', u'122', u'1232', u'125', u'126', u'128', u'12g', u'12th', u'13', u'130', u'135', u'138', u'1396', u'13th', u'14', u'140', u'143', u'1475', u'1492', u'1498', u'14g', u'14th', u'15', u'150', u'1500', u'1500s', u'150th', u'151', u'1522', u'153', u'154', u'15g', u'15th', u'16', u'160', u'1600', u'1618', u'165', u'166', u'169', u'1698', u'16th', u'17', u'170', u'171', u'1716', u'174', u'1744', u'175', u'1789', u'17g', u'17th', u'18', u'180', u'1806', u'1811', u'1819', u'183', u'1843', u'1844', u'185', u'1850s', u'1860s', u'1865', u'1873', u'1875', u'1877', u'1880s', u'1882', u'1889', u'1890s', u'1892', u'1893', u'1895', u'1898', u'189g', u'18th', u'19', u'190', u'1900s', u'1901', u'1902', u'1905', u'1908', u'1909', u'1910', u'1912', u'1918', u'192', u'1920', u'1920s', u'1924', u'1925', u'1927', u'1930s', u'1932', u'1935', u'1936', u'1939', u'1940', u'1940s', u'1944', u'1947', u'1948', u'1950', u'1950s', u'1952', u'1955', u'1959', u'1961', u'1962', u'1964', u'1965', u'1966', u'1967', u'1969', u'197', u'1970', u'1970s', u'1971', u'1972', u'1973', u'1974', u'1975', u'1976', u'1977', u'1978', u'1979', u'1980', u'1980s', u'1981', u'1982', u'1983', u'1984', u'1985', u'1986', u'1987', u'1988', u'1989', u'1990', u'1990s', u'1992', u'1993', u'1994', u'1995', u'1996', u'1997', u'1998', u'1998s', u'1999', u'1999s', u'19th', u'1st', u'20', u'200', u'2000', u'2000s', u'2001', u'2001s', u'2002', u'2002s', u'2003', u'2003s', u'2004', u'2004s', u'2005', u'2005s', u'2006', u'2006s', u'2006\\xef', u'2007', u'2007s', u'2008', u'2008s', u'2009', u'2009s', u'2010', u'2010s', u'2011', u'2011s', u'2012', u'2012s', u'2013', u'2014', u'2014s', u'2015', u'20152019', u'2016', u'2017', u'2018', u'2019', u'2020', u'2020s', u'2021', u'2022', u'2023', u'2024', u'2025', u'2026', u'2027', u'2028', u'2029', u'2030', u'2031', u'2032', u'2033', u'2034', u'2035', u'2037', u'2038', u'2040', u'2042', u'2050', u'209', u'20g', u'20th', u'21', u'212', u'215', u'21st', u'22', u'220', u'223', u'225', u'227', u'23', u'230', u'232', u'233', u'235', u'24', u'240', u'242', u'244', u'247', u'248', u'25', u'250', u'252', u'25th', u'26', u'260', u'263', u'266', u'27', u'270', u'274', u'275', u'277', u'28', u'280', u'283', u'287', u'289', u'29', u'296', u'299', u'2a', u'2l', u'30', u'300', u'30g', u'30th', u'31', u'310', u'317', u'318', u'32', u'325', u'328', u'33', u'330', u'336', u'337', u'33rd', u'34', u'344', u'34south', u'34th', u'35', u'350', u'355', u'36', u'368', u'37', u'370', u'375', u'38', u'389', u'39', u'3g', u'3l', u'3m', u'3rd', u'40', u'400', u'407', u'40th', u'41', u'415', u'416', u'41st', u'42', u'424', u'425', u'43', u'44', u'440', u'45', u'450', u'452', u'46', u'47', u'471', u'475', u'476', u'47g', u'48', u'49', u'496', u'49g', u'4th', u'50', u'500', u'50th', u'51', u'512', u'52', u'523', u'525', u'53', u'530', u'536', u'54', u'54g', u'55', u'550', u'553', u'56', u'563', u'57', u'575', u'58', u'580', u'587', u'59', u'5g', u'5l', u'60', u'600', u'60th', u'61', u'615', u'62', u'624', u'63', u'64', u'640', u'65', u'650', u'651', u'66', u'667', u'67', u'670', u'68', u'680', u'685', u'69', u'690', u'6g', u'70', u'700', u'707', u'70s', u'71', u'72', u'720', u'7200', u'729th', u'73', u'730', u'731', u'738', u'74', u'75', u'750', u'76', u'76s', u'77', u'777', u'78', u'784', u'79', u'795', u'7g', u'7up', u'80', u'800', u'8000', u'806', u'80g', u'80s', u'81', u'813', u'82', u'820', u'83', u'84', u'843', u'85', u'850', u'86', u'869', u'87', u'872', u'875', u'877', u'88', u'89', u'896', u'8g', u'90', u'900', u'904', u'90s', u'91', u'913', u'92', u'93', u'94', u'946', u'95', u'950', u'96', u'960', u'97', u'97s', u'98', u'98s', u'99', u'99s', u'_mocha', u'a1', u'a1611', u'aa', u'aah', u'aand', u'abacela']\n",
      "[u'a1', u'a1611', u'aa', u'aah', u'aand', u'abacela']\n"
     ]
    }
   ],
   "source": [
    "# the numbers\n",
    "words = vect.get_feature_names()\n",
    "print len(words) \n",
    "print words[:500]\n",
    "print words[494:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59991, 1000)\n",
      "(19997, 1000)\n",
      "('The number of word features is', 999)\n",
      "('Training Set Accuracy Score:', 59.133870080512075, '%')\n",
      "('Test Set Accuracy Score:', 47.782167325098762, '%')\n",
      "(59991, 3000)\n",
      "(19997, 3000)\n",
      "('The number of word features is', 2999)\n",
      "('Training Set Accuracy Score:', 73.079295227617479, '%')\n",
      "('Test Set Accuracy Score:', 52.327849177376606, '%')\n",
      "(59991, 5000)\n",
      "(19997, 5000)\n",
      "('The number of word features is', 4999)\n",
      "('Training Set Accuracy Score:', 78.180060342384692, '%')\n",
      "('Test Set Accuracy Score:', 53.438015702355358, '%')\n",
      "(59991, 7000)\n",
      "(19997, 7000)\n",
      "('The number of word features is', 6999)\n",
      "('Training Set Accuracy Score:', 80.603757230251205, '%')\n",
      "('Test Set Accuracy Score:', 53.813071960794126, '%')\n",
      "(59991, 9000)\n",
      "(19997, 9000)\n",
      "('The number of word features is', 8999)\n",
      "('Training Set Accuracy Score:', 81.983964261305857, '%')\n",
      "('Test Set Accuracy Score:', 54.088113216982549, '%')\n"
     ]
    }
   ],
   "source": [
    "# loop over # of max_features\n",
    "max_features = [999, 2999, 4999, 6999, 8999]\n",
    "for m in max_features:\n",
    "    vect_select = CountVectorizer(stop_words = stop, max_features = m)\n",
    "    X_train_select = vect_select.fit_transform(X_train.description)\n",
    "    price = X_train.price.values[:,None]\n",
    "\n",
    "    X_train_select = hstack((X_train_select, price))\n",
    "    print X_train_select.shape\n",
    "\n",
    "    X_test_select = vect_select.transform(X_test.description)\n",
    "    price_test = X_test.price.values[:,None]\n",
    "    X_test_select = hstack((X_test_select, price_test))\n",
    "    print X_test_select.shape\n",
    "\n",
    "    models = {}\n",
    "    for z in wine:\n",
    "        model = LogisticRegression(penalty='l2', C = 1.0)\n",
    "        y = y_train == z\n",
    "        model.fit(X_train_select, y)\n",
    "        models[z] = model\n",
    "        \n",
    "    testing_probs = pd.DataFrame(columns = wine)\n",
    "    training_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "    # predict for training set \n",
    "    for variety in wine:\n",
    "        training_probs[variety] = models[variety].predict_proba(X_train_select)[:,1]\n",
    "    train_predicted_wine = training_probs.idxmax(axis=1)\n",
    "\n",
    "    # predict for test set\n",
    "    for variety in wine:\n",
    "        testing_probs[variety] = models[variety].predict_proba(X_test_select)[:,1]\n",
    "    test_predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "    train_comparison = pd.DataFrame({'actual':y_train.values, 'predicted':train_predicted_wine.values})\n",
    "    test_comparison = pd.DataFrame({'actual':y_test.values, 'predicted':test_predicted_wine.values})\n",
    "    \n",
    "    print('The number of word features is', m)\n",
    "    print('Training Set Accuracy Score:',accuracy_score(train_comparison.actual, train_comparison.predicted)*100,\"%\")\n",
    "    print('Test Set Accuracy Score:',accuracy_score(test_comparison.actual, test_comparison.predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59991, 10000)\n",
      "(19997, 10000)\n",
      "('The number of word features is', 9999)\n",
      "('Training Set Accuracy Score:', 82.394025770532238, '%')\n",
      "('Test Set Accuracy Score:', 54.173125968895327, '%')\n",
      "(59991, 11000)\n",
      "(19997, 11000)\n",
      "('The number of word features is', 10999)\n",
      "('Training Set Accuracy Score:', 82.720741444550015, '%')\n",
      "('Test Set Accuracy Score:', 54.128119217882684, '%')\n",
      "(59991, 12000)\n",
      "(19997, 12000)\n",
      "('The number of word features is', 11999)\n",
      "('Training Set Accuracy Score:', 83.064126285609504, '%')\n",
      "('Test Set Accuracy Score:', 54.173125968895327, '%')\n"
     ]
    }
   ],
   "source": [
    "# loop over # of max_features\n",
    "max_features = [9999, 10999, 11999]\n",
    "for m in max_features:\n",
    "    vect_select = CountVectorizer(stop_words = stop, max_features = m)\n",
    "    X_train_select = vect_select.fit_transform(X_train.description)\n",
    "    price = X_train.price.values[:,None]\n",
    "\n",
    "    X_train_select = hstack((X_train_select, price))\n",
    "    print X_train_select.shape\n",
    "\n",
    "    X_test_select = vect_select.transform(X_test.description)\n",
    "    price_test = X_test.price.values[:,None]\n",
    "    X_test_select = hstack((X_test_select, price_test))\n",
    "    print X_test_select.shape\n",
    "\n",
    "    models = {}\n",
    "    for z in wine:\n",
    "        model = LogisticRegression(penalty='l2', C = 1.0)\n",
    "        y = y_train == z\n",
    "        model.fit(X_train_select, y)\n",
    "        models[z] = model\n",
    "        \n",
    "    testing_probs = pd.DataFrame(columns = wine)\n",
    "    training_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "    # predict for training set \n",
    "    for variety in wine:\n",
    "        training_probs[variety] = models[variety].predict_proba(X_train_select)[:,1]\n",
    "    train_predicted_wine = training_probs.idxmax(axis=1)\n",
    "\n",
    "    # predict for test set\n",
    "    for variety in wine:\n",
    "        testing_probs[variety] = models[variety].predict_proba(X_test_select)[:,1]\n",
    "    test_predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "    train_comparison = pd.DataFrame({'actual':y_train.values, 'predicted':train_predicted_wine.values})\n",
    "    test_comparison = pd.DataFrame({'actual':y_test.values, 'predicted':test_predicted_wine.values})\n",
    "    \n",
    "    print('The number of word features is', m)\n",
    "    print('Training Set Accuracy Score:',accuracy_score(train_comparison.actual, train_comparison.predicted)*100,\"%\")\n",
    "    print('Test Set Accuracy Score:',accuracy_score(test_comparison.actual, test_comparison.predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the optimal params are:\n",
    "### max_features = 8999\n",
    "### C = 1.0\n",
    "### penalty = 'l1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features\n",
    "vect_svm = CountVectorizer(stop_words = stop, max_features = 8999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49662449367405109"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', vect_svm), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(X_train.description, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test.description)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51652747912186825"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', vect_svm),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(X_train.description, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test.description)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune SVM\n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "# doing grid search for SVM# Simila \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],'clf-svm__alpha': (0.001, 0.01, 0.1, 1, 10, 100, 1000)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train.description, y_train)\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  9.79290795,  20.96530461,  10.79501573,  22.88069646,\n",
       "         10.3427763 ,  20.78903762,  11.26008534,  20.85040498,\n",
       "         10.60415967,  19.20159396,   9.75585739,  18.26846464,\n",
       "          9.62529008,  16.31044737]),\n",
       " 'mean_score_time': array([ 3.119229  ,  5.48436499,  3.15463575,  4.37587126,  3.10733581,\n",
       "         5.62439426,  2.77667753,  4.98052597,  2.8078146 ,  4.54225334,\n",
       "         2.55060561,  4.31158137,  2.45465899,  3.31421796]),\n",
       " 'mean_test_score': array([ 0.51349369,  0.51055992,  0.44663366,  0.45176777,  0.35041923,\n",
       "         0.35668684,  0.28974346,  0.29327732,  0.27409111,  0.27619143,\n",
       "         0.04777383,  0.05162441,  0.02855428,  0.02855428]),\n",
       " 'mean_train_score': array([ 0.65336393,  0.70851381,  0.48301429,  0.50220092,  0.35872874,\n",
       "         0.36680507,  0.29263604,  0.29693668,  0.27563354,  0.27804233,\n",
       "         0.04734676,  0.05137934,  0.02854396,  0.02854396]),\n",
       " 'param_clf-svm__alpha': masked_array(data = [0.001 0.001 0.01 0.01 0.1 0.1 1 1 10 10 100 100 1000 1000],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
       "  (1, 1) (1, 2) (1, 1) (1, 2)],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf-svm__alpha': 0.001, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 0.001, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 0.01, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 0.01, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 0.1, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 0.1, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 1, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 1, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 10, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 10, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 100, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 100, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf-svm__alpha': 1000, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf-svm__alpha': 1000, 'vect__ngram_range': (1, 2)}],\n",
       " 'rank_test_score': array([ 1,  2,  4,  3,  6,  5,  8,  7, 10,  9, 12, 11, 13, 13], dtype=int32),\n",
       " 'split0_test_score': array([ 0.51286535,  0.50871846,  0.44391706,  0.44966275,  0.34788908,\n",
       "         0.3537347 ,  0.28863352,  0.2925306 ,  0.27149638,  0.27374469,\n",
       "         0.04166875,  0.04241819,  0.04061954,  0.04061954]),\n",
       " 'split0_train_score': array([ 0.64838903,  0.70509806,  0.48051331,  0.50155093,  0.35808985,\n",
       "         0.36674505,  0.29242546,  0.29675305,  0.27514008,  0.27776666,\n",
       "         0.04157494,  0.04252552,  0.04062437,  0.04062437]),\n",
       " 'split1_test_score': array([ 0.5100755 ,  0.50822541,  0.44547227,  0.45037252,  0.35011751,\n",
       "         0.35581779,  0.29171459,  0.29436472,  0.27706385,  0.27921396,\n",
       "         0.04720236,  0.04930247,  0.04230212,  0.04230212]),\n",
       " 'split1_train_score': array([ 0.65790658,  0.71006701,  0.48824765,  0.50655131,  0.35947189,\n",
       "         0.36719844,  0.29558412,  0.29985997,  0.27935587,  0.28190638,\n",
       "         0.04633427,  0.04830966,  0.04228346,  0.04228346]),\n",
       " 'split2_test_score': array([ 0.51754518,  0.51474195,  0.4505181 ,  0.45527356,  0.35325624,\n",
       "         0.36051459,  0.28888221,  0.29293688,  0.27371477,  0.27561696,\n",
       "         0.05446263,  0.06317265,  0.00270311,  0.00270311]),\n",
       " 'split2_train_score': array([ 0.65379617,  0.71037637,  0.4802819 ,  0.49850052,  0.35862448,\n",
       "         0.36647173,  0.28989854,  0.29419703,  0.27240466,  0.27445394,\n",
       "         0.05413105,  0.06330284,  0.00272405,  0.00272405]),\n",
       " 'std_fit_time': array([ 0.15638381,  1.55671787,  0.33325666,  0.21911296,  1.30255697,\n",
       "         0.41726678,  0.11904249,  0.47041313,  0.26411246,  0.07407148,\n",
       "         0.56139594,  0.1535421 ,  0.48156283,  0.65774205]),\n",
       " 'std_score_time': array([ 0.07462133,  0.61378284,  0.52409927,  0.05339025,  0.33123059,\n",
       "         0.40178443,  0.08175408,  0.29878095,  0.0420213 ,  0.03525274,\n",
       "         0.10696522,  0.08495975,  0.27324027,  0.19645089]),\n",
       " 'std_test_score': array([ 0.00308104,  0.00296178,  0.00281716,  0.00249401,  0.00220144,\n",
       "         0.00283522,  0.00139759,  0.00078664,  0.00228898,  0.00226996,\n",
       "         0.00523856,  0.0086304 ,  0.01827875,  0.01827875]),\n",
       " 'std_train_score': array([ 0.00389753,  0.0024186 ,  0.00370175,  0.0033187 ,  0.00056901,\n",
       "         0.0002997 ,  0.0023259 ,  0.00231553,  0.00285919,  0.00304868,\n",
       "         0.00517577,  0.00875563,  0.01826999,  0.01826999])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf_svm.cv_results_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48592288843326498"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = stop, max_features = 8999)), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(X_train.description, y_train)\n",
    "predicted = text_clf.predict(X_test.description)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45511826774 with alpha value of 0.001\n",
      "0.464169625444 with alpha value of 0.01\n",
      "0.473821073161 with alpha value of 0.1\n",
      "0.485922888433 with alpha value of 1\n",
      "0.376506475971 with alpha value of 10\n",
      "0.301395209281 with alpha value of 100\n",
      "0.280542081312 with alpha value of 1000\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "for a in alpha:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(stop_words = stop, max_features = 8999)), \n",
    "                          ('clf', MultinomialNB(alpha = a))])\n",
    "    text_clf = text_clf.fit(X_train.description, y_train)\n",
    "    predicted = text_clf.predict(X_test.description)\n",
    "    print np.mean(predicted == y_test), 'with alpha value of', a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include More Variables - Slightly worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of countries is 41 points include set([96, 97, 98, 99, 100, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "21\n",
      "The number of wineries included is 13023 The number of wines is 79988\n"
     ]
    }
   ],
   "source": [
    "print 'The number of countries is', len(set(data.country)), 'points include', set(data.points)\n",
    "print len(set(data.points))\n",
    "print 'The number of wineries included is', len(set(data.winery)), 'The number of wines is', len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Canada', 'Turkey', 'Italy', 'Czech Republic', 'Lebanon', 'Lithuania', 'Luxembourg', 'France', 'Slovakia', 'Argentina', 'Israel', 'Australia', 'Mexico', 'Montenegro', 'Slovenia', 'Germany', 'Bosnia and Herzegovina', 'Chile', 'China', 'Serbia', 'Spain', 'Ukraine', 'US-France', 'Georgia', 'Macedonia', 'Moldova', 'Morocco', 'Croatia', 'Japan', 'Switzerland', 'New Zealand', 'Brazil', 'Bulgaria', 'Romania', 'Albania', 'England', 'Portugal', 'South Africa', 'Uruguay', 'India', 'US', 'Austria', 'Greece', 'Hungary', 'South Korea', 'Cyprus']) number of countries 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: Categorical.from_array is deprecated, use Categorical instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: 'labels' is deprecated. Use 'codes' instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])\n"
     ]
    }
   ],
   "source": [
    "# Try including country and points to model\n",
    "print set(data.country), 'number of countries', len(set(data.country))\n",
    "data['country_label'] = pd.Categorical.from_array(data.country).labels\n",
    "print set(data.country_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((66828, 4), (22277, 4), (66828,), (22277,))\n"
     ]
    }
   ],
   "source": [
    "# leave everything except for description out\n",
    "X_new = data.drop(['Unnamed: 0','designation','country','province','region_1','region_2','variety','winery'], axis = 1)\n",
    "y_new = data.variety\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66828, 26359) 66828 66828 66828\n",
      "(22277, 26359) 22277 22277 22277\n"
     ]
    }
   ],
   "source": [
    "# SOME WEIRD DIMENSION ISSUE HERE - GET IT FIXED\n",
    "vect_select = CountVectorizer(stop_words = stop) #MAX FEATURES = SOMETHING\n",
    "\n",
    "X_train_select_desc = vect_select.fit_transform(X_train.description)\n",
    "X_test_select_desc = vect_select.transform(X_test.description)\n",
    "\n",
    "train_price = X_train.price.values[:,None]\n",
    "test_price = X_test.price.values[:,None]\n",
    "\n",
    "# include new features \n",
    "train_points =X_train.points.values[:, None]\n",
    "test_points =X_test.points.values[:, None]\n",
    "\n",
    "# check dimensions\n",
    "print X_train_select_desc.shape,len(train_price), len(train_country), len(train_points)\n",
    "print X_test_select_desc.shape,len(test_price), len(test_country), len(test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hstack - something still wrong with country! points is included \n",
    "X_train_select = hstack((X_train_select_desc, train_price, train_points))\n",
    "X_test_select = hstack((X_test_select_desc, test_price, test_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy Score with points as additional predictor:', 49.104457512232344, '%')\n",
      "               actual           predicted\n",
      "0  Cabernet Sauvignon              Merlot\n",
      "1    Grüner Veltliner    Grüner Veltliner\n",
      "2  Cabernet Sauvignon  Cabernet Sauvignon\n",
      "3   Sangiovese Grosso          Pinot Noir\n",
      "4           Red Blend           Red Blend\n",
      "5          Sangiovese          Pinot Noir\n",
      "6            Riesling            Riesling\n",
      "7          Sangiovese          Sangiovese\n",
      "8    Portuguese White    Portuguese White\n",
      "9      Cabernet Franc  Cabernet Sauvignon\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression again with default params again\n",
    "models = {}\n",
    "for z in wine:\n",
    "    model = LogisticRegression()\n",
    "    y = y_train == z\n",
    "    model.fit(X_train_select, y)\n",
    "    models[z] = model\n",
    "testing_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "# print score\n",
    "for variety in wine:\n",
    "    testing_probs[variety] = models[variety].predict_proba(X_test_select)[:,1]\n",
    "\n",
    "predicted_wine = testing_probs.idxmax(axis=1)\n",
    "comparison = pd.DataFrame({'actual':y_test.values, 'predicted':predicted_wine.values})\n",
    "\n",
    "print('Accuracy Score with points as additional predictor:',accuracy_score(comparison.actual, comparison.predicted)*100,\"%\")\n",
    "print comparison.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy Score with points and country as additional predictor:', 50.608250662117882, '%')\n",
      "               actual         predicted\n",
      "0  Cabernet Sauvignon            Merlot\n",
      "1    Grüner Veltliner  Grüner Veltliner\n",
      "2  Cabernet Sauvignon         Zinfandel\n",
      "3   Sangiovese Grosso        Pinot Noir\n",
      "4           Red Blend         Red Blend\n",
      "5          Sangiovese        Pinot Noir\n",
      "6            Riesling          Riesling\n",
      "7          Sangiovese        Sangiovese\n",
      "8    Portuguese White  Portuguese White\n",
      "9      Cabernet Franc         Red Blend\n"
     ]
    }
   ],
   "source": [
    "# adding country as predictor \n",
    "train_country =X_train.country_label[:, None]\n",
    "test_country =X_test.country_label[:, None]\n",
    "\n",
    "# hstack - something still wrong with country! points is included \n",
    "X_train_select = hstack((X_train_select_desc, train_country, train_price, train_points))\n",
    "X_test_select = hstack((X_test_select_desc, test_country, test_price, test_points))\n",
    "models = {}\n",
    "\n",
    "for z in wine:\n",
    "    model = LogisticRegression()\n",
    "    y = y_train == z\n",
    "    model.fit(X_train_select, y)\n",
    "    models[z] = model\n",
    "testing_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "# print score\n",
    "for variety in wine:\n",
    "    testing_probs[variety] = models[variety].predict_proba(X_test_select)[:,1]\n",
    "\n",
    "predicted_wine = testing_probs.idxmax(axis=1)\n",
    "comparison = pd.DataFrame({'actual':y_test.values, 'predicted':predicted_wine.values})\n",
    "\n",
    "print('Accuracy Score with points and country as additional predictor:',accuracy_score(comparison.actual, comparison.predicted)*100,\"%\")\n",
    "print comparison.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
